# Hybrid GLB Model System - Implementation Plan
**Option A: Lazy Loading with Shared Texture System**

## Overview
Implementation plan for a hybrid GLB model management system that preloads and caches multiple 3D models while using a shared texture system. All models will use the same texture generated by the texture editor, ignoring any embedded textures in GLB files.

## Core Architecture

### 1. Shared Texture Philosophy
- **Single Texture Source**: All GLB models share the same texture canvas from the texture editor
- **GLB Texture Ignored**: Embedded textures in GLB files are completely ignored and never used
- **Clean Materials**: All GLB models get clean MeshLambertMaterial with only the shared texture
- **Unified Appearance**: Different model geometries but consistent texture appearance

### 2. ModelCache System Design

```javascript
class ModelCache {
    constructor(sharedTextureCanvas) {
        this.loadedModels = new Map();          // modelPath -> ModelCacheEntry
        this.visibleModelId = null;             // Currently visible model ID
        this.sharedTextureCanvas = sharedTextureCanvas;  // Single texture source
        this.sharedTexture = null;              // Three.js texture from canvas
        this.modelsContainer = new THREE.Group(); // Container for all cached models
        this.isPreloading = false;
        this.preloadQueue = [];
    }
}

class ModelCacheEntry {
    constructor(modelPath, gltfScene, memoryEstimate) {
        this.modelPath = modelPath;
        this.sceneObject = gltfScene;           // Three.js Group/Object3D
        this.isVisible = false;
        this.lastAccessed = Date.now();
        this.memoryEstimate = memoryEstimate;   // Approximate memory usage
        this.materialsApplied = false;          // Track if shared texture applied
    }
}
```

### 3. Model Loading Strategy

#### Phase 1: Initial Model Load
1. Load first/default GLB model immediately
2. Strip all embedded textures and materials
3. Apply clean materials with shared texture
4. Make model visible
5. Center camera on model

#### Phase 2: Background Preloading (Lazy)
1. After initial model loads successfully, start background preloading
2. Load adjacent models in priority order:
   - Next model in sequence (high priority)
   - Previous model in sequence (high priority)  
   - Remaining models (low priority)
3. Use `requestIdleCallback()` to load during browser idle time
4. Apply same texture cleaning and shared material process

#### Phase 3: Smart Preloading
1. Track user model switching patterns
2. Prioritize frequently accessed models
3. Preload models based on user interaction history
4. Adaptive queue management based on available memory

## Implementation Details

### 1. SceneManager Refactoring

```javascript
class SceneManager {
    constructor(container) {
        // ... existing properties
        this.modelCache = new ModelCache();
        this.modelsGroup = new THREE.Group();
        this.scene.add(this.modelsGroup);
        this.availableModels = [];              // List of all available GLB models
        this.currentModelIndex = 0;
    }

    // Replace existing loadModel method
    async switchToModel(modelPath) {
        const startTime = performance.now();
        
        // Hide currently visible model
        if (this.modelCache.visibleModelId) {
            const currentModel = this.modelCache.getModel(this.modelCache.visibleModelId);
            currentModel.sceneObject.visible = false;
            currentModel.isVisible = false;
        }
        
        // Get or load requested model
        const targetModel = await this.modelCache.getOrLoadModel(modelPath);
        
        // Show target model
        targetModel.sceneObject.visible = true;
        targetModel.isVisible = true;
        targetModel.lastAccessed = Date.now();
        this.modelCache.visibleModelId = modelPath;
        
        // Apply shared texture if not already applied
        if (!targetModel.materialsApplied) {
            this.applySharedTexture(targetModel.sceneObject);
            targetModel.materialsApplied = true;
        }
        
        // Update camera to center on new model
        this.centerCameraOnModel(targetModel.sceneObject);
        
        console.log(`Model switched to ${modelPath} in ${performance.now() - startTime}ms`);
        return targetModel;
    }

    applySharedTexture(modelObject) {
        // Create or update shared texture from canvas
        if (this.layerManager.canvas) {
            if (this.sharedTexture) {
                this.sharedTexture.needsUpdate = true;
            } else {
                this.sharedTexture = new THREE.CanvasTexture(this.layerManager.canvas);
            }
        }

        // Apply shared texture to all materials in model
        modelObject.traverse((child) => {
            if (child.isMesh) {
                // Dispose existing material completely
                if (child.material) {
                    if (Array.isArray(child.material)) {
                        child.material.forEach(mat => mat.dispose());
                    } else {
                        child.material.dispose();
                    }
                }

                // Create clean material with only shared texture
                const cleanMaterial = new THREE.MeshLambertMaterial({
                    map: this.sharedTexture,
                    side: THREE.DoubleSide,
                    transparent: true,
                    alphaTest: 0.01
                });

                child.material = cleanMaterial;
            }
        });
    }
}
```

### 2. Preloading Implementation

```javascript
class ModelPreloader {
    constructor(modelCache, sceneManager) {
        this.modelCache = modelCache;
        this.sceneManager = sceneManager;
        this.preloadQueue = [];
        this.isPreloading = false;
        this.maxConcurrentLoads = 2;
    }

    async startLazyPreloading(availableModels, currentModelIndex) {
        // Build priority queue
        this.buildPreloadQueue(availableModels, currentModelIndex);
        
        // Start preloading in background
        this.preloadInBackground();
    }

    buildPreloadQueue(models, currentIndex) {
        const queue = [];
        
        // High priority: adjacent models
        const adjacentIndexes = [
            currentIndex - 1,  // Previous
            currentIndex + 1   // Next
        ].filter(i => i >= 0 && i < models.length);
        
        adjacentIndexes.forEach(index => {
            if (!this.modelCache.hasModel(models[index].path)) {
                queue.push({
                    path: models[index].path,
                    priority: 'HIGH',
                    index: index
                });
            }
        });
        
        // Low priority: remaining models
        models.forEach((model, index) => {
            if (index !== currentIndex && 
                !adjacentIndexes.includes(index) && 
                !this.modelCache.hasModel(model.path)) {
                queue.push({
                    path: model.path,
                    priority: 'LOW',
                    index: index
                });
            }
        });
        
        // Sort by priority (HIGH first)
        this.preloadQueue = queue.sort((a, b) => 
            a.priority === 'HIGH' ? -1 : (b.priority === 'HIGH' ? 1 : 0)
        );
    }

    async preloadInBackground() {
        if (this.isPreloading) return;
        this.isPreloading = true;

        while (this.preloadQueue.length > 0) {
            const modelToLoad = this.preloadQueue.shift();
            
            try {
                // Use requestIdleCallback for non-blocking preload
                await this.preloadWhenIdle(modelToLoad.path);
                console.log(`âœ… Preloaded model: ${modelToLoad.path}`);
            } catch (error) {
                console.error(`âŒ Failed to preload ${modelToLoad.path}:`, error);
            }
            
            // Yield control to prevent UI blocking
            await this.sleep(100);
        }
        
        this.isPreloading = false;
        console.log('ðŸŽ¯ Background preloading completed');
    }

    preloadWhenIdle(modelPath) {
        return new Promise((resolve, reject) => {
            const loadModel = async () => {
                try {
                    const model = await this.modelCache.loadModelSilently(modelPath);
                    resolve(model);
                } catch (error) {
                    reject(error);
                }
            };

            if ('requestIdleCallback' in window) {
                requestIdleCallback(loadModel, { timeout: 5000 });
            } else {
                // Fallback for browsers without requestIdleCallback
                setTimeout(loadModel, 50);
            }
        });
    }

    sleep(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }
}
```

### 3. Memory Management

```javascript
class ModelMemoryManager {
    constructor(maxCachedModels = 8, maxMemoryMB = 256) {
        this.maxCachedModels = maxCachedModels;
        this.maxMemoryMB = maxMemoryMB;
        this.memoryEstimator = new GLTFMemoryEstimator();
    }

    shouldEvictModels(newModelSize) {
        const currentCount = this.modelCache.size;
        const currentMemory = this.getTotalMemoryUsage();
        
        return (currentCount >= this.maxCachedModels) || 
               (currentMemory + newModelSize > this.maxMemoryMB);
    }

    async evictLeastRecentlyUsed(preserveCurrentModel = true) {
        const models = [...this.modelCache.entries()]
            .map(([path, entry]) => ({ path, ...entry }))
            .sort((a, b) => a.lastAccessed - b.lastAccessed);

        // Keep current visible model and remove oldest unused models
        let modelsToEvict = models.filter(model => 
            !model.isVisible || !preserveCurrentModel
        );

        while (modelsToEvict.length > 0 && this.needsEviction()) {
            const modelToEvict = modelsToEvict.shift();
            await this.evictModel(modelToEvict.path);
            console.log(`ðŸ—‘ï¸ Evicted model: ${modelToEvict.path}`);
        }
    }

    async evictModel(modelPath) {
        const entry = this.modelCache.getModel(modelPath);
        if (!entry) return;

        // Remove from scene
        if (entry.sceneObject.parent) {
            entry.sceneObject.parent.remove(entry.sceneObject);
        }

        // Dispose geometries and materials
        entry.sceneObject.traverse((child) => {
            if (child.isMesh) {
                if (child.geometry) child.geometry.dispose();
                if (child.material) {
                    if (Array.isArray(child.material)) {
                        child.material.forEach(mat => mat.dispose());
                    } else {
                        child.material.dispose();
                    }
                }
            }
        });

        // Remove from cache
        this.modelCache.deleteModel(modelPath);
    }

    getTotalMemoryUsage() {
        let total = 0;
        for (const [, entry] of this.modelCache.entries()) {
            total += entry.memoryEstimate;
        }
        return total;
    }
}

class GLTFMemoryEstimator {
    estimateModelMemory(gltfScene) {
        let totalVertices = 0;
        let totalTextures = 0;
        
        gltfScene.traverse((child) => {
            if (child.isMesh) {
                if (child.geometry) {
                    totalVertices += child.geometry.attributes.position?.count || 0;
                }
                // Note: We ignore embedded textures since we use shared texture
            }
        });
        
        // Rough estimate: 
        // - 36 bytes per vertex (position + normal + uv + indices)
        // - Shared texture doesn't count per model
        const vertexMemoryMB = (totalVertices * 36) / (1024 * 1024);
        
        return Math.max(vertexMemoryMB, 0.5); // Minimum 0.5MB per model
    }
}
```

### 4. Texture Editor Integration

```javascript
// Enhanced LayerManager for multi-model support
class LayerManager {
    constructor(canvas) {
        // ... existing properties
        this.modelCache = null; // Will be injected by SceneManager
    }

    setModelCache(modelCache) {
        this.modelCache = modelCache;
    }

    // Override existing texture update method
    updateTexture() {
        // Create texture from current canvas state
        const texture = new THREE.CanvasTexture(this.canvas);
        texture.needsUpdate = true;

        // Apply to ALL cached models (not just visible one)
        if (this.modelCache) {
            for (const [modelPath, entry] of this.modelCache.entries()) {
                this.applyTextureToModel(entry.sceneObject, texture);
            }
        }

        // Also apply to currently visible model via SceneManager
        if (this.sceneManager) {
            this.sceneManager.setTexture(texture);
        }

        this.onTextureUpdate?.(texture);
    }

    applyTextureToModel(modelObject, texture) {
        modelObject.traverse((child) => {
            if (child.isMesh && child.material) {
                if (Array.isArray(child.material)) {
                    child.material.forEach(mat => {
                        mat.map = texture;
                        mat.needsUpdate = true;
                    });
                } else {
                    child.material.map = texture;
                    child.material.needsUpdate = true;
                }
            }
        });
    }
}
```

### 5. UI Integration

```javascript
class ModelSelector {
    constructor(sceneManager, availableModels) {
        this.sceneManager = sceneManager;
        this.availableModels = availableModels;
        this.currentModelIndex = 0;
        this.preloader = new ModelPreloader(
            sceneManager.modelCache, 
            sceneManager
        );
    }

    async initialize() {
        // Load initial model
        await this.switchToModel(0);
        
        // Start background preloading
        this.preloader.startLazyPreloading(
            this.availableModels, 
            this.currentModelIndex
        );
        
        // Create UI controls
        this.createModelSelectorUI();
    }

    async switchToModel(modelIndex) {
        if (modelIndex < 0 || modelIndex >= this.availableModels.length) {
            return;
        }

        const model = this.availableModels[modelIndex];
        const wasInstant = this.sceneManager.modelCache.hasModel(model.path);
        
        try {
            await this.sceneManager.switchToModel(model.path);
            this.currentModelIndex = modelIndex;
            this.updateUISelection(modelIndex);
            
            console.log(`Switched to model ${modelIndex}: ${model.name} ${wasInstant ? '(instant)' : '(loaded)'}`);
        } catch (error) {
            console.error('Failed to switch model:', error);
            this.showError(`Failed to load model: ${model.name}`);
        }
    }

    createModelSelectorUI() {
        const selectorHTML = `
            <div class="model-selector">
                <div class="model-controls">
                    <button id="prev-model" ${this.currentModelIndex === 0 ? 'disabled' : ''}>
                        â—€ Previous
                    </button>
                    <select id="model-dropdown">
                        ${this.availableModels.map((model, index) => 
                            `<option value="${index}" ${index === this.currentModelIndex ? 'selected' : ''}>
                                ${model.name}
                            </option>`
                        ).join('')}
                    </select>
                    <button id="next-model" ${this.currentModelIndex === this.availableModels.length - 1 ? 'disabled' : ''}>
                        Next â–¶
                    </button>
                </div>
                <div class="preload-status">
                    <div class="cache-info">
                        <span id="cached-count">Loading...</span>
                        <span id="memory-usage">Memory: 0MB</span>
                    </div>
                </div>
            </div>
        `;
        
        // Insert into existing UI
        const controlsContainer = document.querySelector('#controls-container');
        controlsContainer.insertAdjacentHTML('afterbegin', selectorHTML);
        
        // Attach event listeners
        this.attachEventListeners();
        
        // Update status periodically
        setInterval(() => this.updateCacheStatus(), 2000);
    }

    attachEventListeners() {
        document.getElementById('prev-model')?.addEventListener('click', () => {
            this.switchToModel(this.currentModelIndex - 1);
        });

        document.getElementById('next-model')?.addEventListener('click', () => {
            this.switchToModel(this.currentModelIndex + 1);
        });

        document.getElementById('model-dropdown')?.addEventListener('change', (e) => {
            this.switchToModel(parseInt(e.target.value));
        });
    }

    updateCacheStatus() {
        const cacheSize = this.sceneManager.modelCache.size;
        const totalModels = this.availableModels.length;
        const memoryUsage = this.sceneManager.memoryManager.getTotalMemoryUsage();
        
        document.getElementById('cached-count').textContent = 
            `Cached: ${cacheSize}/${totalModels}`;
        document.getElementById('memory-usage').textContent = 
            `Memory: ${memoryUsage.toFixed(1)}MB`;
    }
}
```

## Implementation Timeline

### Week 1: Core Foundation
- [ ] Refactor SceneManager to use ModelCache
- [ ] Implement basic model caching with visibility switching
- [ ] Create clean material system (ignore GLB textures)
- [ ] Basic model switching functionality

### Week 2: Preloading System  
- [ ] Implement ModelPreloader with lazy loading
- [ ] Add requestIdleCallback integration
- [ ] Build priority-based preloading queue
- [ ] Test background preloading performance

### Week 3: Memory Management
- [ ] Implement ModelMemoryManager
- [ ] Add LRU eviction system
- [ ] Create memory estimation for GLB models
- [ ] Add configurable memory limits

### Week 4: UI Integration
- [ ] Create ModelSelector UI component
- [ ] Add model switching controls
- [ ] Implement cache status display
- [ ] Add error handling and user feedback

## Configuration Options

```javascript
const HYBRID_MODEL_CONFIG = {
    // Memory Management
    maxCachedModels: 8,           // Maximum models to keep in memory
    maxMemoryMB: 256,             // Memory limit for model cache
    
    // Preloading Strategy
    preloadPriority: 'LAZY',      // LAZY, IMMEDIATE, ADAPTIVE
    preloadAdjacentCount: 2,      // Number of adjacent models to preload
    useIdleCallback: true,        // Use requestIdleCallback for preloading
    
    // Performance
    maxConcurrentLoads: 2,        // Maximum simultaneous model loads
    preloadDelay: 100,            // Delay between preload operations (ms)
    
    // Texture System
    ignoreGLBTextures: true,      // Always ignore embedded textures
    sharedTextureFormat: 'CANVAS', // CANVAS, WEBGL_TEXTURE
    textureUpdateAll: true        // Update all cached models when texture changes
};
```

## Benefits Summary

âœ… **Instant Model Switching**: 0ms switching for cached models  
âœ… **Shared Texture System**: Consistent appearance across all models  
âœ… **Memory Efficient**: LRU cache with configurable limits  
âœ… **Progressive Loading**: Start fast, enhance in background  
âœ… **Clean Architecture**: GLB models provide geometry only  
âœ… **Scalable**: Handles unlimited models with smart eviction  
âœ… **User Experience**: No interruption to texture editing workflow  

## Technical Notes

### GLB Texture Handling
- All embedded textures in GLB files are ignored completely
- Materials are replaced with clean MeshLambertMaterial
- Only the shared texture from texture editor is used
- This ensures consistent appearance across different models

### Performance Characteristics
- First model load: ~200-2000ms (normal GLB loading)
- Cached model switch: ~1-5ms (visibility toggle)
- Background preloading: No impact on UI responsiveness
- Memory usage: Predictable and configurable

### Error Handling
- Failed model loads don't break the system
- Fallback to previous model if switching fails
- Graceful degradation when memory limits exceeded
- User feedback for loading states and errors